"""
SeekingAlpha çš„å·¥å…·å‡½æ•°
1. è·å–ç¾è‚¡æ–°é—»
"""
import sys
from pathlib import Path

_CURRENT_FILE = Path(__file__).resolve()
_CONTEST_TRADE_DIR = _CURRENT_FILE.parents[1]
if str(_CONTEST_TRADE_DIR) not in sys.path:
    sys.path.insert(0, str(_CONTEST_TRADE_DIR))

import os
import json
import pandas as pd
import requests
import re
import html
from pathlib import Path
from datetime import datetime, timedelta
from functools import lru_cache
import hashlib
import pickle
import time
from typing import List
from config.config import cfg

DEFAULT_SEEKINGALPHA_CACHE_DIR = Path(__file__).parent / "seekingalpha_cache"

class CachedSeekingAlphaClient:
    def __init__(self, cache_dir=None, api_key=None):
        if not cache_dir:
            self.cache_dir = DEFAULT_SEEKINGALPHA_CACHE_DIR
        else:
            self.cache_dir = Path(cache_dir)
        if not self.cache_dir.exists():
            self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # è·å–APIå¯†é’¥
        if not api_key:
            api_key = cfg.seekingalpha_key
        
        self.api_key = api_key
        self.base_url = "https://seeking-alpha.p.rapidapi.com"
        self.rate_limit_delay = 0.2  # APIé™åˆ¶ï¼Œæ¯ç§’æœ€å¤š5æ¬¡è¯·æ±‚

    def run(self, endpoint: str, params: dict, verbose: bool = False):
        """
        è¿è¡ŒSeekingAlpha APIè¯·æ±‚å¹¶ç¼“å­˜ç»“æœ
        
        Args:
            endpoint: APIç«¯ç‚¹è·¯å¾„
            params: è¯·æ±‚å‚æ•°
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        """
        params_str = json.dumps(params, sort_keys=True)
        return self.run_with_cache(endpoint, params_str, verbose)
    
    def run_with_cache(self, endpoint: str, params_str: str, verbose: bool = False):
        params = json.loads(params_str)
        
        # åˆ›å»ºç¼“å­˜æ–‡ä»¶è·¯å¾„
        endpoint_clean = endpoint.replace('/', '_').replace('?', '_').replace(':', '_').replace('=', '_').replace('&', '_').replace('-', '_').lstrip('_')
        cache_key = f"{endpoint_clean}_{hashlib.md5(params_str.encode()).hexdigest()}"
        endpoint_cache_dir = self.cache_dir / endpoint_clean
        if not endpoint_cache_dir.exists():
            endpoint_cache_dir.mkdir(parents=True, exist_ok=True)
        cache_file = endpoint_cache_dir / f"{cache_key}.pkl"
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        if cache_file.exists():
            if verbose:
                print(f"ğŸ“ ä»ç¼“å­˜åŠ è½½: {cache_file}")
            with open(cache_file, "rb") as f:
                return pickle.load(f)
        else:
            if verbose:
                print(f"ğŸŒ APIè¯·æ±‚: {endpoint} å‚æ•°: {params}")
            
            # é™åˆ¶APIè¯·æ±‚é¢‘ç‡
            time.sleep(self.rate_limit_delay)
            
            try:
                # æ„å»ºå®Œæ•´URL
                url = f"{self.base_url}{endpoint}"
                headers = {
                    "x-rapidapi-key": self.api_key,
                    "x-rapidapi-host": "seeking-alpha.p.rapidapi.com"
                }
                # å‘é€è¯·æ±‚
                response = requests.get(url, headers=headers, params=params)
                response.raise_for_status()
                result = response.json()
                # ä¿å­˜åˆ°ç¼“å­˜
                if verbose:
                    print(f"ğŸ’¾ ä¿å­˜ç¼“å­˜: {cache_file}")
                with open(cache_file, "wb") as f:
                    pickle.dump(result, f)
                
                return result
            except Exception as e:
                if verbose:
                    print(f"âŒ APIè¯·æ±‚å¤±è´¥: {e}")
                raise e


    def get_stock_news(self, symbol: str = None, limit: int = 40, verbose: bool = False):
        """
        è·å–è‚¡ç¥¨æ–°é—»
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç  (å¯é€‰)
            limit: è¿”å›æ–°é—»æ•°é‡
        """
        params = {'size': limit}
        if symbol:
            endpoint = f'/news/v2/list-by-symbol?id={symbol}'
        else:
            endpoint = '/news/v2/list?category=market-news::all'
        return self.run(endpoint, params, verbose=verbose)


    def get_news_content(self, id: str, verbose: bool = False):
        """
        è·å–æ–°é—»å…·ä½“å†…å®¹
        """
        endpoint = f'/news/get-details?id={id}'
        return self.run(endpoint, {}, verbose=verbose)
    

    def add_news_with_content(self, news_list: list, verbose: bool = False):
        """
        ä¸ºæ–°é—»åˆ—è¡¨ä¸­çš„æ¯æ¡æ–°é—»æ·»åŠ å†…å®¹
        
        Args:
            news_list: æ–°é—»åˆ—è¡¨
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
            
        Returns:
            list: åŒ…å«å†…å®¹çš„æ–°é—»åˆ—è¡¨
        """
        completed_news = []
        for item in news_list:
            try:
                # è·å–æ–°é—»å†…å®¹
                content_result = self.get_news_content(item["id"], verbose)
                
                # æå–å†…å®¹æ–‡æœ¬
                if isinstance(content_result, dict):
                    content = content_result.get("data", {}).get("attributes", {}).get("content", "")
                    if not content:
                        content = content_result.get("content", "")
                else:
                    content = str(content_result) if content_result else ""
                
                # æ¸…ç†HTMLå†…å®¹
                cleaned_content = clean_html_content(content)
                
                # åˆ›å»ºæ–°çš„æ–°é—»é¡¹
                completed_item = item.copy()
                completed_item["content"] = cleaned_content
                completed_news.append(completed_item)
                
                if verbose:
                    print(f"âœ… å·²è·å–æ–°é—»å†…å®¹: {item.get('title', 'Unknown')[:50]}...")
                    
            except Exception as e:
                if verbose:
                    print(f"âŒ è·å–æ–°é—»å†…å®¹å¤±è´¥ (ID: {item.get('id', 'Unknown')}): {e}")
                # å³ä½¿è·å–å†…å®¹å¤±è´¥ï¼Œä¹Ÿä¿ç•™åŸå§‹æ–°é—»é¡¹
                completed_item = item.copy()
                completed_item["content"] = ""
                completed_news.append(completed_item)
        
        return completed_news

# åˆ›å»ºå…¨å±€ç¼“å­˜å®¢æˆ·ç«¯
seekingalpha_cached = CachedSeekingAlphaClient()

def process_raw_result(raw_result: dict):
    base = "https://seekingalpha.com"
    processed = []
    for item in raw_result.get("data", []):
        attributes = item.get("attributes", {})
        links = item.get("links", {})

        publish_on = attributes.get("publishOn")
        pub_time = None
        if publish_on:
            try:
                dt = datetime.fromisoformat(publish_on)
                pub_time = dt.strftime("%Y-%m-%d %H:%M:%S")
            except Exception:
                pub_time = publish_on

        processed.append({
            "id": item.get("id", ""),
            "title": attributes.get("title", ""),
            "publishedDate": pub_time,
            "url": (base + links.get("self")) if links.get("self") else None
        })
    return processed


def clean_html_content(content: str):
    text_no_html = re.sub(r"<[^>]+>", "", content)
    clean_content = html.unescape(text_no_html)
    clean_content = re.sub(r"\s+", " ", clean_content).strip()
    return clean_content

@lru_cache(maxsize=1000)
def get_us_stock_news(symbol: str = None, limit: int = 40, verbose: bool = False):
    """è·å–ç¾è‚¡æ–°é—»"""
    # è·å–æ–°é—»åˆ—è¡¨
    result = seekingalpha_cached.get_stock_news(symbol, limit, verbose)
    result = process_raw_result(result)
    
    # ä¸ºæ¯æ¡æ–°é—»æ·»åŠ å†…å®¹
    if result:
        result = seekingalpha_cached.add_news_with_content(result, verbose)
        # print(result)
        # with open("seekingalpha_news.json", "w") as f:
        #     json.dump(result, f, indent=4)
    if result:
        df = pd.DataFrame(result)
        if not df.empty and 'publishedDate' in df.columns:
            df['publishedDate'] = pd.to_datetime(df['publishedDate'])
            df = df.sort_values('publishedDate', ascending=False).reset_index(drop=True)
            return df
    return pd.DataFrame()


if __name__ == "__main__":
    df = get_us_stock_news()
    print(df)