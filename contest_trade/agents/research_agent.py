"""
A general react-agent for analysing.
"""

import os
import re
import sys
import json
import yaml
import textwrap
import asyncio
from loguru import logger
from typing import List, Dict, Any, Optional, TypedDict
from datetime import datetime
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from langgraph.graph import StateGraph, END
from utils.llm_utils import count_tokens

from agents.prompts import prompt_for_research_plan, prompt_for_research_choose_tool, prompt_for_research_write_result, prompt_for_research_invest_task, prompt_for_research_invest_output_format
from models.llm_model import GLOBAL_LLM, GLOBAL_THINKING_LLM
from tools.tool_utils import ToolManager, ToolManagerConfig
from config.config import cfg, PROJECT_ROOT
from langchain_core.runnables import RunnableConfig
from utils.market_manager import GLOBAL_MARKET_MANAGER

@dataclass
class ResearchAgentInput:
    """Agentè¾“å…¥"""
    background_information: str
    trigger_time: str


@dataclass
class ResearchAgentOutput:
    """Agentå†³ç­–ç»“æœ"""
    task: str  # ä»»åŠ¡
    trigger_time: str
    background_information: str
    belief: str
    final_result: str  # æŠ¥å‘Š
    final_result_thinking: str  # æŠ¥å‘Šæ€è€ƒ

    def to_dict(self):
        return {
            "task": self.task,
            "trigger_time": self.trigger_time,
            "background_information": self.background_information,
            "belief": self.belief,
            "final_result": self.final_result,
            "final_result_thinking": self.final_result_thinking
        }

@dataclass
class ResearchAgentConfig:
    """Agenté…ç½®"""
    agent_name: str
    belief: str
    max_react_step: int
    tool_config: ToolManagerConfig
    output_language: str
    plan: bool
    react: bool

    def __init__(self, agent_name: str = "research_agent", belief: str = ""):
        self.agent_name = agent_name
        self.belief = belief
        self.max_react_step = cfg.research_agent_config["max_react_step"]
        self.tool_config = ToolManagerConfig(cfg.research_agent_config["tools"])
        self.output_language = cfg.system_language
        if 'plan' in cfg.research_agent_config:
            self.plan = cfg.research_agent_config["plan"]
        else:
            self.plan = True
        if 'react' in cfg.research_agent_config:
            self.react = cfg.research_agent_config["react"]
        else:
            self.react = True

class ResearchAgentState(TypedDict):
    """LangGraph AgentçŠ¶æ€"""
    # åŸºæœ¬ä¿¡æ¯
    task: str = ""
    trigger_time: str = ""
    belief: str = ""
    background_information: str = ""
    
    # ä¸Šä¸‹æ–‡å’Œé¢„ç®—
    plan_result: str = ""
    tool_call_context: str = ""
    
    # æ€è€ƒå’Œå†³ç­–
    selected_tool: dict = {}
    tool_call_count: int = 0
    tool_call_results: list = []
    step_count: int = 0
    
    # æœ€ç»ˆç»“æœ
    final_result: str = ""
    final_result_thinking: str = ""
    result: ResearchAgentOutput = None


class ResearchAgent:
    """åŸºäºLangGraphçš„æŠ•èµ„å†³ç­–Agent"""
    
    def __init__(self, config: ResearchAgentConfig):
        self.config = config
        self.tool_manager = ToolManager(self.config.tool_config)
        self.app = self._build_graph()
        self.plan = self.config.plan
        self.react = self.config.react


        self.signal_dir = PROJECT_ROOT / "agents_workspace" / "reports" / self.config.agent_name
        if not self.signal_dir.exists():
            self.signal_dir.mkdir(parents=True, exist_ok=True)


    def _build_graph(self) -> StateGraph:
        """æ„å»ºLangGraphçŠ¶æ€å›¾"""
        workflow = StateGraph(ResearchAgentState)
        workflow.add_node("init_signal_dir", self._init_signal_dir)
        workflow.add_node("recompute_signal", self._recompute_signal)
        workflow.add_node("init_data", self._init_data)
        workflow.add_node("plan", self._plan)
        workflow.add_node("tool_selection", self._tool_selection)
        workflow.add_node("call_tool", self._call_tool)
        workflow.add_node("write_result", self._write_result)
        workflow.add_node("submit_result", self._submit_result)
        
        # å®šä¹‰è¾¹
        workflow.set_entry_point("init_signal_dir")
        workflow.add_conditional_edges("init_signal_dir",
            self._recompute_signal,
            {
                "yes": "init_data",
                "no": "submit_result"
            })
        workflow.add_edge("recompute_signal", "init_data")
        workflow.add_conditional_edges("init_data",
            self._need_plan,
            {
                "yes": "plan",
                "no": "tool_selection"
            }
        )
        workflow.add_edge("plan", "tool_selection")
        workflow.add_conditional_edges("tool_selection",
            self._enough_information,
            {
                "enough_information": "write_result",
                "not_enough_information": "call_tool"
            })
        workflow.add_edge("call_tool", "tool_selection")
        workflow.add_edge("write_result", "submit_result")
        workflow.add_edge("submit_result", END)
        return workflow.compile()

    async def _init_signal_dir(self, state: ResearchAgentState) -> ResearchAgentState:
        """try to load signal from file"""
        try:
            signal_file = self.signal_dir / f'{state["trigger_time"].replace(" ", "_").replace(":", "-")}.json'
            if signal_file.exists():
                with open(signal_file, 'r', encoding='utf-8') as f:
                    signal_data = json.load(f)
                state["result"] = ResearchAgentOutput(**signal_data)
        except Exception as e:
            print(f"Error loading signal from file: {e}")
            import traceback
            traceback.print_exc()
        return state
    
    async def _recompute_signal(self, state: ResearchAgentState):
        """recompute signal"""
        if state["result"]:
            print(f"Signal already exists for {state['trigger_time']}, skipping recompute")
            return "no"
        else:
            print(f"Signal does not exist for {state['trigger_time']}, recomputing signal")
            return "yes"

    async def _init_data(self, state: ResearchAgentState) -> ResearchAgentState:
        """åˆå§‹åŒ–æ•°æ®"""
        state["tool_call_count"] = 0
        return state

    async def _need_plan(self, state: ResearchAgentState) -> str:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è§„åˆ’"""
        if self.plan:
            return "yes"
        else:
            return "no"

    async def _plan(self, state: ResearchAgentState) -> ResearchAgentState:
        """è§„åˆ’ä»»åŠ¡"""
        try:
            if not self.plan:
                state["plan_result"] = ""
                return state
            prompt = prompt_for_research_plan.format(
                current_time=state["trigger_time"],
                task=state["task"],
                background_information=state["background_information"],
                tools_info=self.tool_manager.build_toolcall_context(),
                output_language=self.config.output_language,
            )
            messages = [{"role": "user", "content": prompt}]
            plan_result = await GLOBAL_LLM.a_run(messages, verbose=True, thinking=False, max_retries=10)
            plan_result = plan_result.content
            state["plan_result"] = plan_result.strip()
        except Exception as e:
            logger.error(f"Error in plan: {e}")
            state["plan_result"] = ""
        return state

    async def _tool_selection(self, state: ResearchAgentState) -> ResearchAgentState:
        """é€‰æ‹©å·¥å…·"""
        if not self.react:
            state["selected_tool"] = {"tool_name": "final_report"}
            return state

        prompt = prompt_for_research_choose_tool.format(
            current_time=state["trigger_time"],
            task=state["task"],
            plan=state["plan_result"],
            background_information=state["background_information"],
            tool_call_context=state["tool_call_context"],
            tools_info=self.tool_manager.build_toolcall_context(),
            output_language=self.config.output_language,
        )
        try:
            next_tool = await self.tool_manager.select_tool_by_llm(
                prompt=prompt,
            )
        except Exception as e:
            logger.error(f"Error in tool_selection: {e}")
            next_tool = {"error": str(e)}
        state["selected_tool"] = next_tool
        return state


    async def _enough_information(self, state: ResearchAgentState) -> str:
        """åˆ¤æ–­æ˜¯å¦è¶³å¤Ÿä¿¡æ¯"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æˆåŠŸçš„å·¥å…·è°ƒç”¨
            tool_call_context = state["tool_call_context"]
            has_successful_calls = False
            if tool_call_context:
                import json
                lines = tool_call_context.strip().split('\n')
                for line in lines:
                    if line.strip():
                        try:
                            call_data = json.loads(line)
                            if call_data.get("tool_result", {}).get("status") == "success":
                                has_successful_calls = True
                                break
                        except:
                            continue
            
            print(f"ğŸ” [Step {state['tool_call_count']}] Checking if enough information:")
            print(f"   - Tool calls made: {state['tool_call_count']}/{self.config.max_react_step}")
            print(f"   - Has successful calls: {has_successful_calls}")
            
            estimated_context = prompt_for_research_write_result.format(
                current_time=state["trigger_time"],
                task=state["task"],
                background_information=state["background_information"],
                plan=state["plan_result"],
                tool_call_context=state["tool_call_context"],
                tools_info=self.tool_manager.build_toolcall_context(),
                output_format=self.get_output_format(),
                output_language=self.config.output_language,
            )

            if count_tokens(estimated_context) > 128000:
                print("   - Stopping: Context too long")
                return "enough_information"

            selected_tool = state["selected_tool"]
            if "error" in selected_tool:
                print("   - Continuing: Tool selection has error")
                return "not_enough_information"
            
            # å¦‚æœè°ƒç”¨äº†final_reportæˆ–è¾¾åˆ°æœ€å¤§æ­¥æ•°ï¼Œç»“æŸ
            if selected_tool["tool_name"] == "final_report" or \
                state["tool_call_count"] >= self.config.max_react_step:
                print(f"   - Stopping: final_report called or max steps reached")
                return "enough_information"
                
            # å¦‚æœæ²¡æœ‰æˆåŠŸçš„å·¥å…·è°ƒç”¨ä¸”è¿˜æœ‰å‰©ä½™æ­¥æ•°ï¼Œç»§ç»­å°è¯•
            if not has_successful_calls and state["tool_call_count"] < self.config.max_react_step:
                print("   - Continuing: No successful tool calls yet, need more data")
                return "not_enough_information"
                
            print("   - Continuing: Default behavior")
            return "not_enough_information"
            
        except Exception as e:
            logger.error(f"Error in enough_information: {e}")
            return "not_enough_information"


    async def _call_tool(self, state: ResearchAgentState) -> ResearchAgentState:
        """è°ƒç”¨å·¥å…·"""
        selected_tool = state["selected_tool"]
        try:
            print(f'ğŸ”§ [Step {state["tool_call_count"] + 1}] Begin to call tool: {selected_tool}')
            tool_name = selected_tool["tool_name"]
            tool_args = selected_tool["properties"]
            tool_result = await self.tool_manager.call_tool(tool_name, tool_args, state["trigger_time"])
            
            # è¯¦ç»†æ£€æŸ¥å·¥å…·è°ƒç”¨ç»“æœ
            if tool_result is None:
                print(f"âš ï¸  WARNING: Tool '{tool_name}' returned None - possible tool failure")
                tool_result = {"error": f"Tool {tool_name} returned None", "status": "failed", "has_data": False}
            elif isinstance(tool_result, str) and len(tool_result.strip()) == 0:
                print(f"âš ï¸  WARNING: Tool '{tool_name}' returned empty string")
                tool_result = {"error": f"Tool {tool_name} returned empty result", "status": "failed", "has_data": False}
            elif isinstance(tool_result, dict) and "error" in tool_result:
                print(f"âŒ Tool '{tool_name}' returned error: {tool_result.get('error')}")
                tool_result["status"] = "failed"
                tool_result["has_data"] = False
            else:
                print(f"âœ… Tool '{tool_name}' executed successfully")
                print(f"ğŸ“Š Result type: {type(tool_result)}, length: {len(str(tool_result))}")
                # ç¡®ä¿æœ‰æ•°æ®æ ‡è®°
                if isinstance(tool_result, dict):
                    tool_result["status"] = "success"
                    tool_result["has_data"] = True
                else:
                    tool_result = {"data": tool_result, "status": "success", "has_data": True}
                
        except Exception as e:
            print(f"âŒ CRITICAL: Tool '{selected_tool.get('tool_name', 'unknown')}' execution failed: {e}")
            logger.error(f"Error in call_tool: {e}")
            tool_result = {"error": str(e), "status": "failed", "has_data": False}
        
        state["tool_call_count"] += 1
        state["tool_call_context"] += json.dumps({"tool_called":selected_tool,\
                                            "tool_result":tool_result}, ensure_ascii=False) + "\n"
        return state


    def _calculate_enhanced_data_quality(self, tool_call_context: str) -> dict:
        """
        è®¡ç®—å¢å¼ºçš„æ•°æ®è´¨é‡æŒ‡æ ‡
        """
        import json
        
        # åŸºç¡€æˆåŠŸç‡è®¡ç®—
        successful_calls = 0
        total_calls = 0
        successful_tools = []
        data_sources = set()
        tool_results = []
        content_quality_score = 0.0
        
        if tool_call_context:
            lines = tool_call_context.strip().split('\n')
            for line in lines:
                if line.strip():
                    try:
                        call_data = json.loads(line)
                        total_calls += 1
                        
                        if call_data.get("tool_result", {}).get("status") == "success":
                            successful_calls += 1
                            tool_name = call_data.get("tool_called", {}).get("tool_name", "unknown")
                            successful_tools.append(tool_name)
                            data_sources.add(tool_name)
                            
                            # è¯„ä¼°å†…å®¹è´¨é‡
                            result = call_data.get("tool_result", {})
                            result_length = len(str(result))
                            
                            if result_length > 100:  # æœ‰å®è´¨æ€§å†…å®¹
                                content_quality_score += 1
                            elif result_length > 50:  # æœ‰åŸºæœ¬å†…å®¹
                                content_quality_score += 0.5
                                
                            tool_results.append({
                                'tool': tool_name,
                                'result_length': result_length,
                                'has_structured_data': isinstance(result.get('data'), (dict, list))
                            })
                    except:
                        continue
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        success_rate = successful_calls / total_calls if total_calls > 0 else 0
        content_quality = content_quality_score / total_calls if total_calls > 0 else 0
        source_diversity = len(data_sources) / max(total_calls, 1)
        data_freshness = min(successful_calls / max(total_calls, 1), 1.0)
        
        # ç»¼åˆè´¨é‡åˆ†æ•°
        overall_score = (
            success_rate * 0.4 +           # æˆåŠŸç‡æƒé‡40%
            content_quality * 0.3 +        # å†…å®¹è´¨é‡æƒé‡30%
            source_diversity * 0.2 +       # æ•°æ®æºå¤šæ ·æ€§æƒé‡20%
            data_freshness * 0.1           # æ•°æ®æ—¶æ•ˆæ€§æƒé‡10%
        )
        
        return {
            'success_rate': success_rate,
            'content_quality': content_quality,
            'source_diversity': source_diversity,
            'data_freshness': data_freshness,
            'overall_score': overall_score,
            'total_calls': total_calls,
            'successful_calls': successful_calls,
            'unique_sources': len(data_sources),
            'successful_tools': successful_tools,
            'tool_results': tool_results
        }
    
    def _generate_enhanced_hallucination_warning(self, quality_metrics: dict) -> str:
        """
        åŸºäºå¢å¼ºçš„æ•°æ®è´¨é‡æŒ‡æ ‡ç”Ÿæˆç²¾ç¡®çš„å¹»è§‰é˜²æŠ¤è­¦å‘Š
        """
        overall_score = quality_metrics['overall_score']
        success_rate = quality_metrics['success_rate']
        content_quality = quality_metrics['content_quality']
        source_diversity = quality_metrics['source_diversity']
        successful_calls = quality_metrics['successful_calls']
        
        if overall_score < 0.3:
            return f"""

ğŸš¨ ä¸¥é‡è­¦å‘Šï¼šæ•°æ®è´¨é‡æä½ (æ€»åˆ†: {overall_score:.2f})
**ä¸¥æ ¼é™åˆ¶è¦æ±‚**ï¼š
- æˆåŠŸç‡: {success_rate:.1%} | å†…å®¹è´¨é‡: {content_quality:.2f} | æ•°æ®æºå¤šæ ·æ€§: {source_diversity:.2f}
- **ç»å¯¹ç¦æ­¢**ï¼šç¼–é€ ä»»ä½•æ•°æ®ã€æ¨æµ‹æœªéªŒè¯ä¿¡æ¯ã€ä½¿ç”¨æ¨¡ç³Šè¡¨è¿°
- **å¿…é¡»åšåˆ°**ï¼šæ¯ä¸ªç»“è®ºéƒ½æ ‡æ³¨"[æ•°æ®ä¸è¶³]"ï¼Œæ¦‚ç‡è¯„ä¼°ä¸å¾—è¶…è¿‡30%
- **å¼ºåˆ¶è¦æ±‚**ï¼šåœ¨limitationsä¸­è¯¦ç»†è¯´æ˜æ•°æ®ç¼ºå¤±æƒ…å†µ
- **è¾“å‡ºé™åˆ¶**ï¼šåªèƒ½åŸºäº{successful_calls}ä¸ªæˆåŠŸå·¥å…·è°ƒç”¨çš„ç¡®åˆ‡ç»“æœ
"""
        elif overall_score < 0.5:
            return f"""

âš ï¸ é‡è¦è­¦å‘Šï¼šæ•°æ®è´¨é‡è¾ƒä½ (æ€»åˆ†: {overall_score:.2f})
**é™åˆ¶è¦æ±‚**ï¼š
- æˆåŠŸç‡: {success_rate:.1%} | å†…å®¹è´¨é‡: {content_quality:.2f} | æ•°æ®æºå¤šæ ·æ€§: {source_diversity:.2f}
- **ä¸¥æ ¼ç¦æ­¢**ï¼šç¼–é€ æ•°æ®ã€è¿‡åº¦æ¨æµ‹ã€ä½¿ç”¨ä¸ç¡®å®šçš„è¡¨è¿°å¦‚"å¯èƒ½"ã€"æ®è¯´"
- **å¿…é¡»æ ‡æ³¨**ï¼šæ¯ä¸ªè¯æ®çš„ç¡®å®šæ€§çº§åˆ«ä¸å¾—è¶…è¿‡60%
- **å¼ºåˆ¶å¼•ç”¨**ï¼šä½¿ç”¨æ ‡å‡†æ ¼å¼ [å·¥å…·å|æ—¶é—´|å…·ä½“æ•°å€¼] å¼•ç”¨æ‰€æœ‰æ•°æ®
- **æ¦‚ç‡é™åˆ¶**ï¼šæœ€ç»ˆæ¦‚ç‡è¯„ä¼°ä¸å¾—è¶…è¿‡50%
"""
        elif overall_score < 0.7:
            return f"""

âš ï¸ æ³¨æ„ï¼šæ•°æ®å®Œæ•´æ€§ä¸­ç­‰ (æ€»åˆ†: {overall_score:.2f})
**è°¨æ…è¦æ±‚**ï¼š
- æˆåŠŸç‡: {success_rate:.1%} | å†…å®¹è´¨é‡: {content_quality:.2f} | æ•°æ®æºå¤šæ ·æ€§: {source_diversity:.2f}
- **ç¦æ­¢è¡Œä¸º**ï¼šç¼–é€ å…·ä½“æ•°å€¼ã€æ··æ·†äº‹å®ä¸æ¨è®º
- **å¿…é¡»åŒºåˆ†**ï¼šæ˜ç¡®æ ‡æ³¨å“ªäº›æ˜¯ç›´æ¥æ•°æ®ï¼Œå“ªäº›æ˜¯åŸºäºæ•°æ®çš„æ¨è®º
- **å¼•ç”¨è¦æ±‚**ï¼šæ‰€æœ‰å…³é”®æ•°æ®å¿…é¡»ä½¿ç”¨æ ‡å‡†å¼•ç”¨æ ¼å¼
- **ä¸ç¡®å®šæ€§**ï¼šé€‚å½“æ ‡æ³¨ä¸ç¡®å®šæ€§çº§åˆ«ï¼Œä¿æŒè°¨æ…æ€åº¦
"""
        else:
            return f"""

âœ… æ•°æ®è´¨é‡è‰¯å¥½ (æ€»åˆ†: {overall_score:.2f})
**æ ‡å‡†è¦æ±‚**ï¼š
- æˆåŠŸç‡: {success_rate:.1%} | å†…å®¹è´¨é‡: {content_quality:.2f} | æ•°æ®æºå¤šæ ·æ€§: {source_diversity:.2f}
- **åŸºæœ¬åŸåˆ™**ï¼šç¡®ä¿æ‰€æœ‰ç»“è®ºéƒ½æœ‰æ˜ç¡®çš„æ•°æ®æ”¯æ’‘
- **å¼•ç”¨æ ‡å‡†**ï¼šä½¿ç”¨è§„èŒƒçš„æ•°æ®å¼•ç”¨æ ¼å¼
- **å®¢è§‚æ€§**ï¼šåŒºåˆ†å®¢è§‚äº‹å®å’Œä¸»è§‚åˆ†æ
- **å®Œæ•´æ€§**ï¼šåœ¨data_quality_assessmentä¸­æä¾›è¯¦ç»†çš„è´¨é‡è¯„ä¼°
"""

    async def _write_result(self, state: ResearchAgentState) -> ResearchAgentState:
        """å†™ç»“æœ - å¢å¼ºç‰ˆå¹»è§‰æ£€æµ‹"""
        try:
            # å¢å¼ºç‰ˆå¹»è§‰æ£€æµ‹ï¼šå¤šå±‚æ•°æ®éªŒè¯
            tool_call_context = state["tool_call_context"]
            print(f"ğŸ“Š Analyzing tool call context length: {len(tool_call_context)}")
            
            # è®¡ç®—å¢å¼ºçš„æ•°æ®è´¨é‡æŒ‡æ ‡
            quality_metrics = self._calculate_enhanced_data_quality(tool_call_context)
            
            print(f"ğŸ“ˆ Enhanced Data Quality Metrics:")
            print(f"   - Overall Score: {quality_metrics['overall_score']:.2f}")
            print(f"   - Success Rate: {quality_metrics['success_rate']:.1%}")
            print(f"   - Content Quality: {quality_metrics['content_quality']:.2f}")
            print(f"   - Source Diversity: {quality_metrics['source_diversity']:.2f}")
            print(f"   - Successful Tools: {quality_metrics['successful_tools']}")
            
            # ç”Ÿæˆç²¾ç¡®çš„å¹»è§‰é˜²æŠ¤è­¦å‘Š
            hallucination_warning = self._generate_enhanced_hallucination_warning(quality_metrics)
            
            # å°†è´¨é‡æŒ‡æ ‡æ·»åŠ åˆ°çŠ¶æ€ä¸­ï¼Œä¾›åç»­ä½¿ç”¨
            state["data_quality_metrics"] = quality_metrics
            
            if self.get_output_format() is None:
                state["output_format"] = "xxxx"
            prompt = prompt_for_research_write_result.format(
                current_time=state["trigger_time"],
                task=state["task"],
                background_information=state["background_information"],
                plan=state["plan_result"],
                tool_call_context=state["tool_call_context"] + hallucination_warning,
                tools_info=self.tool_manager.build_toolcall_context(),
                output_format=self.get_output_format(),
                output_language=self.config.output_language,
            )
            messages = [{"role": "user", "content": prompt}]
            if cfg.llm_thinking.get("api_key", None):
                result_result = await GLOBAL_THINKING_LLM.a_run(messages, verbose=False, thinking=True, max_retries=5)
            else:
                result_result = await GLOBAL_LLM.a_run(messages, verbose=False, thinking=False, max_retries=5)
            state["final_result"] = result_result.content
            state["final_result_thinking"] = result_result.reasoning_content
            
            # åˆ›å»º ResearchAgentOutput å¯¹è±¡
            state["result"] = ResearchAgentOutput(
                task=state["task"],
                trigger_time=state["trigger_time"],
                background_information=state["background_information"],
                belief=state["belief"],
                final_result=state["final_result"],
                final_result_thinking=state["final_result_thinking"]
            )
        except Exception as e:
            logger.error(f"Error in write_report: {e}")
            state["final_result"] = ""
            state["result"] = None
        return state
    
    async def _submit_result(self, state: ResearchAgentState) -> ResearchAgentState:
        """Write the result to a file"""
        try:
            signal_file = self.signal_dir / f'{state["trigger_time"].replace(" ", "_").replace(":", "-")}.json'
            with open(signal_file, 'w', encoding='utf-8') as f:
                json.dump(state["result"].to_dict(), f, ensure_ascii=False, indent=4)
            print(f"Research result saved to {signal_file}")
        except Exception as e:
            print(f"Error writing result: {e}")
            import traceback
            traceback.print_exc()
        return state

    def build_background_information(self, trigger_time: str, belief: str, factors: List):
        """æ„å»ºèƒŒæ™¯ä¿¡æ¯"""
        
        global_market_information = ""
        for factor in factors:
            # å¤„ç†ä¸åŒçš„factorç±»å‹
            if hasattr(factor, 'result') and factor.result:
                factor_output = factor.result
                factor_name = factor_output.agent_name
                factor_update_time = factor_output.trigger_time
                factor_context = factor_output.context_string
            elif hasattr(factor, 'agent_name'):
                factor_name = factor.agent_name
                factor_update_time = factor.trigger_time
                factor_context = factor.context_string
            elif isinstance(factor, dict):
                factor_name = factor.get('agent_name', 'unknown')
                factor_update_time = factor.get('trigger_time', trigger_time)
                factor_context = factor.get('context_string', '')
            else:
                continue
                
            global_market_information += textwrap.dedent(f"""
            <global_summary>
            <source>{factor_name}</source>
            <timestamp>{factor_update_time}</timestamp>
            <content>{factor_context}</content>
            </global_summary>
            """)

        target_market = GLOBAL_MARKET_MANAGER.get_target_symbol_context(trigger_time)
        
        background_information_format = textwrap.dedent("""
        <market_information>
        {global_market_information}
        </market_information>

        <target_market>
        {target_market}
        </target_market>

        <your_belief>
        {belief}
        </your_belief>
        """)
        return background_information_format.format(
            global_market_information=global_market_information,
            target_market=target_market,
            belief=belief
        )

    def get_invest_prompt(self):
        """è·å–æŠ•èµ„æç¤º"""
        return prompt_for_research_invest_task

    def get_output_format(self):
        """è·å–è¾“å‡ºæ ¼å¼"""
        return prompt_for_research_invest_output_format

    async def run_with_monitoring_events(self, input: ResearchAgentInput, config: RunnableConfig = None) -> ResearchAgentOutput:
        """ä½¿ç”¨äº‹ä»¶æµç›‘æ§è¿è¡ŒAgentï¼Œè¿”å›äº‹ä»¶æµ"""
        initial_state = ResearchAgentState(
            trigger_time=input.trigger_time,
            task=self.get_invest_prompt(),
            belief=self.config.belief,
            background_information=input.background_information,
            plan_result="",
            tool_call_context="",
            selected_tool={},
            tool_call_count=0,
            step_count=0,
            final_result="",
            final_result_thinking="",
            result=None
        )
        print(f"ğŸš€ Research Agent Starting - {input.trigger_time}")
        async for event in self.app.astream_events(initial_state, version="v2", config=config or RunnableConfig(recursion_limit=50)):
            yield event

    async def run_with_monitoring(self, input: ResearchAgentInput) -> ResearchAgentOutput:
        """ä½¿ç”¨äº‹ä»¶æµç›‘æ§è¿è¡ŒAgent"""
        print(f"ğŸš€ Research Agent Starting - {input.trigger_time}")
        final_result = None
        async for event in self.run_with_monitoring_events(input, RunnableConfig(recursion_limit=50)):
            event_type = event["event"]
            if event_type == "on_chain_start":
                node_name = event["name"]
                if node_name != "__start__":  # å¿½ç•¥å¼€å§‹äº‹ä»¶
                    print(f"ğŸ”„ Starting: {node_name}")
                
            elif event_type == "on_chain_end":
                node_name = event["name"]
                if node_name != "__start__":  # å¿½ç•¥å¼€å§‹äº‹ä»¶
                    print(f"âœ… Completed: {node_name}")
                    if node_name == "submit_result":
                        final_state = event.get("data", {}).get("output", None)
                        if final_state and "result" in final_state and final_state["result"]:
                            return final_state["result"]
        print(f"âœ¨ Research Agent Completed")
        return final_result
        

if __name__ == "__main__":
    # init instance
    config = ResearchAgentConfig(
        agent_name="research_agent_vtes11",
    )
    agent = ResearchAgent(config)

    #task = input("è¯·è¾“å…¥ä»»åŠ¡: ")
    task = "ç¾è‚¡ç§‘æŠ€é¾™å¤´è‚¡æœ‰å“ªäº›"
    task = "è‹¹æœå…¬å¸çš„æœ€è¿‘3å¤©è‚¡ä»·"
    agent_input = ResearchAgentInput(
        trigger_time="2025-07-09 09:00:00",
        background_information="123123123"
    )
    agent_output = asyncio.run(agent.run_with_monitoring(agent_input))
    print(agent_output.to_dict())

